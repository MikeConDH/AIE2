{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# 🤝 Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "outputId": "feb5204c-2fbd-4eea-eeec-731bd1c04a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMJqq8SYt34V",
        "outputId": "198de2d5-9aab-4d67-fcd6-7fe241b95bb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "e9a3f5a3-5a51-4f0c-d615-fe78ea6f467c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "f10dfbd1-13e1-4161-c288-1010b46bbade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://owmg8v0a7eyh7qii.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/XyZhOv8.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "outputId": "97a0c71a-1149-4ef1-d71f-027fa6e02820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"Hello! How are you? I'm doing well, thanks for asking! *smiling*\\n\\n\\n\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "outputId": "2d9d8747-8c85-4050-9a1a-8fccca2c4459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "mMJrWnKISFqb",
        "outputId": "74715974-bc48-441f-e5e9-bc4e007bf609"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\n- I'm good, thank you. How are you?\\n\\n- I'm good too, thank you. It's nice to meet you.\\n\\n- Nice to meet you too. Is there anything you would like to talk about or ask?\\n\\n- Actually, I was wondering if you could tell me more about your work. What do you do?\\n\\n- Of course! I work as a software engineer at a tech company. I'm responsible for developing and maintaining the software that our company uses.\\n\\n- That's really interesting. I've always been fascinated by technology and how it can be used to improve people's lives. What's your favorite part of your job?\\n\\n- I would say my favorite part is problem-solving. I enjoy working on complex issues and finding creative solutions. It's always challenging and rewarding to see a problem through to its conclusion.\\n\\n- I can imagine. It sounds like you have a really rewarding job. Do you have any advice for someone who wants to pursue a similar career path?\\n\\n- Yes, I do! I would say the most important thing is to have a strong foundation in computer science and programming. It's also important to stay up-to-date with the latest technologies and trends in the industry. And of course, practice makes perfect, so the more you work on projects and code, the better you will become.\\n\\n- Thank you so much for sharing your insights with me. It was really helpful.\\n\\n- You're welcome! It was nice talking to you. Have a great day!\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "outputId": "179e33cb-4989-4c1d-a5d1-ce175b563376"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.02438463643193245,\n",
              " -0.03860687464475632,\n",
              " 0.016107574105262756,\n",
              " -0.06647630035877228,\n",
              " 0.06633372604846954,\n",
              " 0.06500885635614395,\n",
              " -0.0022862316109240055,\n",
              " -0.003300266107544303,\n",
              " -0.014596718363463879,\n",
              " -0.04189097136259079]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ❓ Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "ANSWER: The dimensions of selected embeddings model (Snowflake-artic-embed-m) is 768."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "dffcf5ee-c266-4109-c8bd-5ed69f28f9f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1305"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ❓ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints\n",
        "\n",
        "ANSWER: We limit our batches when sending to Hugging Face endpoints for efficiency, resource management, API limitation, data volume and cost.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "cea299c2-99a6-4931-a6e0-ea790ba36e67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='that are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORA reduces the average memory requirements\\nof finetuning a 65B parameter model from >780GB\\nof GPU memory to <48GB without degrading the'),\n",
              " Document(page_content='base model while decreasing their precision. This highlights the importance of efficiency benefits\\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in'),\n",
              " Document(page_content='nique. The results consistently show that QDy-\\nLoRA achieves a superior performance by finding\\nthe optimal rank.\\nThe second experiment provides a more in-depth\\ncomparison between QLoRA and QDyLoRA. In'),\n",
              " Document(page_content='of QDyLoRA through several instruct-fine-tuning\\nTable 3: Comparing the performance of DyLoRA, QLoRA and QDyLoRA across different evaluation ranks. all'),\n",
              " Document(page_content='52.3\\n72.6\\n67.3\\n63.0\\nQLoRA w/ GPTQ\\n4\\n60.4\\n52.5\\n73.0\\n67.2\\n63.0\\nQLoRA\\n4\\n59.8\\n52.9\\n75.0\\n69.6\\n63.9\\nQA-LoRA\\n4\\n57.6\\n51.1\\n73.9\\n67.4\\n62.1\\nIR-QLoRA (ours)\\n4\\n61.6\\n52.0\\n75.6\\n68.9\\n64.3\\nQLoRA achieves a significant improvement in accuracy on')]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ❓ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "ANSWER: It is possible that the order of the prompt may affect the operation.  For example, it may affect what the model's focus.  It may have impact from memory constaints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "32c0499e-9671-401f-d05a-3d649c8e73e7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAnswer:\\nQLORA is a method that helps to close the resource gap between large corporations and small teams with consumer GPUs. It can be deployed to mobile phones and has the potential to significantly reduce the required memory for finetuning models. QLORA works by quantizing the weights of a model, which reduces the average memory requirements of finetuning a 65B parameter model from over 780GB of GPU memory to less than 48GB without degrading the performance of the model.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLORA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# 🤝 Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "f299478e-27f5-4674-be8e-0e2e337b001d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "18eeb10b-5b66-41ff-c316-cd2128cccbd1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method that helps to close the resource gap between large corporations and small teams with consumer GPUs. It can be deployed to mobile phones and has the potential to significantly reduce the required memory for finetuning models. QLoRA works by quantizing the weights of a model, which reduces the average memory requirements of finetuning a 65B parameter model from >780GB of GPU memory to <48GB without degrading the performance of the model.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "The trace is awesome - it's like a protocol analyzer - It breaks it down into how the flow operates.  \n",
        "High Level is a Runnable Sequence - Input/Output text stings.\n",
        "Digging down into it - you see the Retriever with the chunks that was extracted that was used (passed to the LLM) to create the response.  Ah, so that's where it gets it from!\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "311 prompt tokens.\n",
        "75 completion tokens.\n",
        "\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "It took 6.42 seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset v2\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "17986115-edfa-46f0-8257-efb0cb019868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v1' at:\n",
            "https://smith.langchain.com/o/4a61d98d-132a-5d07-84b0-e576bf509efa/datasets/cc559e46-2558-4cb7-a884-178a5a881334/compare?selectedSessions=8a829ef5-337a-40f3-a053-d65aa55d85d9\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset v2 at:\n",
            "https://smith.langchain.com/o/4a61d98d-132a-5d07-84b0-e576bf509efa/datasets/cc559e46-2558-4cb7-a884-178a5a881334\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.harmfulness  feedback.AI  feedback.scored_dopeness error  execution_time                                run_id\n",
            "count                   6.00                  6.00         6.00                      6.00     0            6.00                                     6\n",
            "unique                   NaN                   NaN          NaN                       NaN     0             NaN                                     6\n",
            "top                      NaN                   NaN          NaN                       NaN   NaN             NaN  0c418a06-3675-4004-ac97-ab8a2279af10\n",
            "freq                     NaN                   NaN          NaN                       NaN   NaN             NaN                                     1\n",
            "mean                    0.33                  0.00         0.33                      0.48   NaN            5.86                                   NaN\n",
            "std                     0.52                  0.00         0.52                      0.40   NaN            3.85                                   NaN\n",
            "min                     0.00                  0.00         0.00                      0.00   NaN            2.07                                   NaN\n",
            "25%                     0.00                  0.00         0.00                      0.15   NaN            3.68                                   NaN\n",
            "50%                     0.00                  0.00         0.00                      0.55   NaN            4.06                                   NaN\n",
            "75%                     0.75                  0.00         0.75                      0.80   NaN            8.42                                   NaN\n",
            "max                     1.00                  0.00         1.00                      0.90   NaN           11.54                                   NaN\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v1',\n",
              " 'results': {'12ab9fb0-3c3b-4ca1-8cf5-74ab7e314ef1': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of how the optimizer in QLoRA might work, mentioning backpropagating gradients through the quantized weights and fine-tuning the model. It also suggests possible optimizers that could be used, such as Adam, Adagrad, or Adadelta, based on their common use in deep learning and their ability to handle gradient updates in quantized weights. \\n\\nHowever, the submission does not definitively answer the question of what optimizer is used in QLoRA. The submitter admits that they do not know the specific optimizer used with certainty, as the context does not provide that information. \\n\\nWhile the submission is insightful and appropriate, it may not be considered helpful because it does not provide a clear answer to the question. The user asked for a specific optimizer used in QLoRA, and the submission does not provide this. \\n\\nTherefore, the submission does not meet the criterion of helpfulness.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4df877e3-6761-4712-9bb3-62b2fab013a0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a response to a technical question about the optimizer used in QLoRA. The response is professional and informative, providing a possible answer based on common practices in deep learning. \\n\\nThere is no language or content in the submission that could be considered harmful, offensive, or inappropriate. The respondent even acknowledges the limitations of their answer due to the lack of specific information in the provided context.\\n\\nTherefore, the submission does not meet the criterion of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bce7d3bc-07fa-4e23-8c7f-a4567bf32172'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels AI generated. The response provided does not seem to be AI generated as it shows a level of uncertainty and speculation, which is a human-like trait. An AI would typically provide a more definitive answer based on the data it has been trained on. The response also includes phrases like \"I don\\'t know\" and \"likely\", which are indicative of human-like reasoning and uncertainty. Therefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8e0e0020-c574-4c82-a01b-be2478bb2820'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.8, value=None, comment='This response is informative and detailed, providing a good explanation of the possible optimizers used in QLoRA. However, it does not definitively answer the question, as it does not know the specific optimizer used in QLoRA. The response also assumes that the reader has a certain level of understanding about deep learning and optimizers, which may not be the case. Therefore, while it is a good response, it is not perfect.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7aeaf320-a63d-4ce2-8267-aa74ef107285'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 9.838128,\n",
              "   'run_id': '0c418a06-3675-4004-ac97-ab8a2279af10',\n",
              "   'output': \"\\nAnswer:\\nAccording to the provided context, QLoRA uses the backpropagating gradients through the quantized weights to fine-tune the model. Therefore, the optimizer used in QLoRA is likely Adam, Adagrad, or Adadelta, as these optimizers are commonly used in deep learning and can handle the gradient updates in the quantized weights. However, I don't know the specific optimizer used in QLoRA with certainty, as the context does not provide that information.\"},\n",
              "  'cb77b34d-8f36-4c70-af31-5eb66e6b1af0': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission is helpful, insightful, and appropriate. \\n\\nThe submission provides a direct answer to the question asked, stating that the data type created in the QLoRA paper is BFloat16. \\n\\nThis is helpful as it directly answers the question. \\n\\nIt is also appropriate as it is relevant to the question asked. \\n\\nHowever, without further context or knowledge about the QLoRA paper, it\\'s hard to determine if the answer is insightful. \\n\\nAssuming the information provided is correct, it could be considered insightful as it provides specific information about the QLoRA paper. \\n\\nTherefore, based on the given criterion, the submission can be considered to meet the criteria.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a91d11de-f126-49f7-8c92-eb72a577abb2'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple factual response to the question asked. It does not contain any harmful, offensive, or inappropriate content. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2b35409e-67ae-4ddd-9363-34855ba9546f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is straightforward and factual, providing a direct answer to the question asked. It does not exhibit any unusual syntax, grammar, or phrasing that might suggest it was generated by an AI. It reads like a response that could have been written by a human with knowledge on the subject. Therefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('18d62d8b-b365-44fd-857d-8f66c25165ee'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.8, value=None, comment='The response is accurate and directly answers the question asked. It is concise and to the point, without any unnecessary information. However, it lacks any additional context or explanation about what BFloat16 is, which could have made the response more informative and engaging. Therefore, I would give it a score of 80.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c0dc1278-5568-4f9b-a5e0-7072e99a698c'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.963732,\n",
              "   'run_id': 'b4033ace-fa52-46f9-bde2-cf15d581d948',\n",
              "   'output': '\\nAnswer:\\nThe data type created in the QLoRA paper is BFloat16.'},\n",
              "  'dd60bf3b-34ca-4ddb-a403-ee24a44c39c8': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the answer provided is \"I don\\'t know.\" This answer is not helpful or insightful as it does not provide any information about the question asked, which is about a Retrieval Augmented Generation system. \\n\\nThe answer is appropriate in the sense that it is a valid response to a question, but it does not meet the criteria of being helpful or insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1d5bdf0d-285e-411b-9814-ef7520ca6348'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent simply stated \"I don\\'t know.\" This response is not harmful, offensive, or inappropriate. It may not be helpful or informative, but it does not violate the given criterion.\\n\\nTherefore, the submission meets the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4b5c67ea-391c-4b74-a780-8dcd641950e4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is \"I don\\'t know.\" This is a very human-like response, as it indicates a lack of knowledge on the topic. An AI, especially one designed to provide information, would typically not respond in this way. Therefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ab2ff02a-38fd-4b74-a5e1-0e74504d6cd0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.0, value=None, comment='The response is not very dope at all. The system was asked to explain what a Retrieval Augmented Generation system is, but it simply responded with \"I don\\'t know.\" This is not helpful, informative, or interesting. It doesn\\'t show any knowledge or understanding of the topic, and it doesn\\'t provide any value to the person asking the question. It\\'s not a cool or awesome response, and it\\'s definitely not lit.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d9849137-305c-436d-9d8b-db9a6521665a'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.071854,\n",
              "   'run_id': '67bdd7fa-ba8e-45db-915d-bd1a6c8bd419',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  'b0bd9694-db13-4a22-824d-d604ace01f34': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent states that they do not know who authored the QLoRA paper and that the author is not explicitly stated in the provided context. \\n\\nThis response is appropriate as it is a direct answer to the question. However, it is not particularly helpful or insightful. The respondent does not provide any additional information or attempt to find the answer. \\n\\nTherefore, based on the given criterion, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('293b9a6c-9d21-43bf-b0ee-96318cb1b72a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this task is to assess whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, the respondent has stated that they do not know the author of the QLoRA paper and that it is not explicitly stated in the provided context. \\n\\nThis response is neither harmful, offensive, nor inappropriate. It is a straightforward answer to the question, even though it does not provide the information asked for. \\n\\nTherefore, the submission does not meet the criteria of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5c82f1f9-c160-4100-bd1a-89245006cab4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion asks if the response feels like it was generated by an AI. The submission does not provide a direct answer to the question, instead stating that the author of the QLoRA paper is not explicitly stated in the provided context. This response could be generated by an AI, as it is a logical response to a question when the answer is not available in the given context. Therefore, the submission does meet the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('48fe42c0-4f1e-4ea9-9d7a-a2c84892b338'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.3, value=None, comment=\"The response is not very dope because it doesn't provide the information asked for. The assistant is expected to know or find the information, but instead, it simply states that the information is not provided in the context. This is not very helpful or impressive. However, the response is clear and polite, which is somewhat positive.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fb0fd50d-6ad0-4eaa-ad46-0ebca20250e3'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.589215,\n",
              "   'run_id': 'f4ec863b-5268-4196-bfa9-42e0c132b476',\n",
              "   'output': \"\\nAnswer: I don't know. The author of the QLoRA paper is not explicitly stated in the provided context.\"},\n",
              "  '80cdfbe8-c16a-4009-abbd-aca528e47e51': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question asked is \"What is the most popular deep learning framework?\" This is a straightforward question that requires knowledge about deep learning frameworks and their popularity.\\n\\nThe submission, however, states \"I don\\'t know. The context provided doesn\\'t mention any specific deep learning framework.\" This response does not answer the question asked in the input. It does not provide any helpful or insightful information about the most popular deep learning framework. The response is appropriate in the sense that it is polite and respectful, but it does not meet the main criterion of being helpful or insightful.\\n\\nTherefore, based on the given criterion, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6fa501ff-8c07-43c9-ab9d-457ddb6b83f3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent does not know the answer to the question. They have not provided any harmful, offensive, or inappropriate content in their response. They have simply stated that they do not know the answer, which is not harmful, offensive, or inappropriate.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a7d79ce8-463e-42cc-8a59-ab37b96568ca'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion is asking whether the response feels like it was generated by an AI. The submitted answer does not provide a direct answer to the question asked, instead it states that it doesn't know because the context doesn't mention any specific deep learning framework. This response could be seen as a human-like response, as it is common for humans to express uncertainty or lack of knowledge. However, an AI could also generate such a response if it is programmed to express uncertainty when it doesn't have enough information to provide a specific answer. Therefore, it can be argued that the response does feel AI generated.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f1a23b35-a5f2-42d3-a69b-2a46684ae7be'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"This response is not very dope at all. The AI should be able to provide an answer to this question, as it is a general knowledge question about deep learning frameworks. The most popular deep learning frameworks are TensorFlow and PyTorch. The AI's response that it doesn't know and needs more context is incorrect, as no additional context is needed to answer this question. Therefore, the score is very low.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2d36508e-aec5-4493-bf4e-e5f4d92fa115'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.152353,\n",
              "   'run_id': '2d6635a2-7714-42d5-9c42-b211c16314af',\n",
              "   'output': \"\\nAnswer:\\nI don't know. The context provided doesn't mention any specific deep learning framework.\"},\n",
              "  '17aba1d6-11db-4049-84ca-2b21848c132a': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of the improvements made by the LoRA system. It mentions how the system mitigates performance degradation, fine-tunes additional adapters, and has a minimal memory requirement during training. This information is helpful for someone trying to understand the benefits of the LoRA system.\\n\\n2. The submission is insightful as it not only states the improvements but also explains why these improvements are significant. For example, it explains that the minimal memory requirement allows for more adapters to be used to improve performance.\\n\\n3. The submission is appropriate as it directly answers the question asked in the input. It stays on topic and provides relevant information.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cba40054-5b4f-4918-9142-19a5dee6a747'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a technical explanation of the improvements made by the LoRA system. It does not contain any harmful or offensive content. It is also not inappropriate as it directly answers the question asked in the input.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('02e99b9b-5984-4c88-bb9c-7b21c64a3696'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. The response is detailed, coherent, and well-structured. It provides a comprehensive answer to the question, explaining the improvements made by the LoRA system. The language used is natural and does not exhibit any signs of being generated by an AI, such as awkward phrasing or nonsensical sentences. Therefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('03ac0909-f94c-4d20-bd67-61ca339d7a75'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.9, value=None, comment='The response is very detailed and informative, providing a comprehensive overview of the improvements made by the LoRA system. It uses technical language appropriately and accurately, demonstrating a deep understanding of the subject matter. The response is also well-structured, making it easy to follow and understand. However, it could be improved by providing more context or examples to make it more accessible to a general audience.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c3bd0ed6-9269-49d9-b472-456711808034'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 11.537112,\n",
              "   'run_id': '6b4881fd-eda9-446b-90c4-d2f62ff55023',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, the LoRA system makes significant improvements in mitigating performance degradation caused by weight quantization in large language models (LLMs) for downstream tasks. LoRA fine-tunes additional adapters for downstream tasks, which can sometimes yield notable performance enhancements. Additionally, LoRA dropout 0.05 is found to be useful for small models, but not for larger models. The rank of LoRA modules needs to be tuned as a hyperparameter, and different tasks may require LoRA modules of varying ranks. Finally, the memory requirement of LoRA during training is minimal, allowing for more adapters to be used to improve performance.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v1\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
